{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190fa43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, START, END, add_messages\n",
    "from langgraph.types import Command, interrupt\n",
    "from typing import Annotated, TypedDict, cast\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import uuid\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    ")\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    linkedin_topic: str\n",
    "    generated_post: Annotated[list[str], add_messages]\n",
    "    human_feedback: Annotated[list[str], add_messages]\n",
    "\n",
    "\n",
    "def model_node(state: State):\n",
    "    print(\"Generating LinkedIn post...\")\n",
    "    linkedin_topic = state[\"linkedin_topic\"]\n",
    "    feedback = state[\"human_feedback\"] if \"human_feedback\" in state else [\n",
    "        \"no feedback yet\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    linkedin_topic: {linkedin_topic}\n",
    "    human_feedback: {feedback[-1] if feedback else \"no feedback yet\"}\n",
    "\n",
    "    Generate a Structured and well written LinkedIn post based on the topic and feedback.\n",
    "    The post should be engaging, professional, and suitable for a LinkedIn audience.\n",
    "\n",
    "    consider previous feedback to improve the post.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([SystemMessage(content=\"You are a expert LinkedIn content generator.\"),\n",
    "                           HumanMessage(content=prompt)])\n",
    "\n",
    "    generated_post = response.content\n",
    "    print(f\"Generated LinkedIn post: {generated_post}\")\n",
    "\n",
    "    return {\n",
    "        \"generated_post\": [AIMessage(content=generated_post)],\n",
    "        \"human_feedback\": feedback\n",
    "    }\n",
    "\n",
    "\n",
    "def human_node(state: State):\n",
    "    \"\"\"Human feedback node for LinkedIn post generation. loops back to the model node.\"\"\"\n",
    "    generated_post = state[\"generated_post\"]\n",
    "\n",
    "    user_feedback = interrupt(\n",
    "        {\n",
    "            \"generated_post\": generated_post,\n",
    "            \"message\": \"Please provide feedback on the generated LinkedIn post. or type 'done' to finish.\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"User feedback: {user_feedback}\")\n",
    "\n",
    "    if user_feedback.lower() == \"done\":\n",
    "        return Command(\n",
    "            update={\n",
    "                \"human_feedback\": state[\"human_feedback\"] + [\"finalized\"],\n",
    "\n",
    "            },\n",
    "            goto=\"end_node\"\n",
    "        )\n",
    "\n",
    "    return Command(\n",
    "        update={\n",
    "            \"human_feedback\": state[\"human_feedback\"] + [HumanMessage(content=user_feedback)],\n",
    "        },\n",
    "        goto=\"model_node\"\n",
    "    )\n",
    "\n",
    "\n",
    "def end_node(state: State):\n",
    "    \"\"\"End node to finalize the LinkedIn post generation.\"\"\"\n",
    "    print(\"Finalizing LinkedIn post generation...\")\n",
    "    final_post = state[\"generated_post\"][-1] if state[\"generated_post\"] else \"No post generated.\"\n",
    "    print(f\"Final LinkedIn post: {final_post}\")\n",
    "    print(\"Final Human feedback:\", state[\"human_feedback\"])\n",
    "\n",
    "    return {\n",
    "        \"generated_post\": state[\"generated_post\"],\n",
    "        \"human_feedback\": state[\"human_feedback\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# Define the state graph\n",
    "state_graph = StateGraph(State)\n",
    "state_graph.add_node(\"model_node\", model_node)\n",
    "state_graph.add_node(\"human_node\", human_node)\n",
    "state_graph.add_node(\"end_node\", end_node)\n",
    "\n",
    "# Define the edges between nodes\n",
    "state_graph.add_edge(START, \"model_node\")\n",
    "state_graph.add_edge(\"model_node\", \"human_node\")\n",
    "\n",
    "state_graph.set_finish_point(\"end_node\")\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "graph = state_graph.compile(checkpointer=checkpointer)\n",
    "\n",
    "# try:\n",
    "#     display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "# except Exception:\n",
    "#     # This requires some extra dependencies and is optional\n",
    "#     pass\n",
    "\n",
    "# Enable the interrupt feature\n",
    "thread_config = RunnableConfig(\n",
    "    {\"configurable\": {\n",
    "        \"thread_id\": str(uuid.uuid4()),\n",
    "    }}\n",
    ")\n",
    "\n",
    "linkedin_topic = input(\"Enter the LinkedIn topic: \")\n",
    "initial_state = {\n",
    "    \"linkedin_topic\": linkedin_topic,\n",
    "    \"generated_post\": [],\n",
    "    \"human_feedback\": [],\n",
    "}\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    initial_state,\n",
    "    config=thread_config,\n",
    "):\n",
    "    for node_id, value in chunk.items():\n",
    "        if node_id == \"__interrupt__\":\n",
    "            while True:\n",
    "                user_input = input(\n",
    "                    \"provide feedback or type 'done' to finish: \")\n",
    "                # resume the graph execution with the user input\n",
    "                graph.stream(Command(resume=user_input),\n",
    "                             config=thread_config)\n",
    "\n",
    "                # Exit loop if user says done\n",
    "                if user_input.lower() == \"done\":\n",
    "                    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
